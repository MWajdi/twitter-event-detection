{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/wajdi.maatouk/Tweet-event-prediction/myenv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-07 10:20:22.852260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733563223.227070  833060 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733563223.327716  833060 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-07 10:20:24.304303: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model and tokenizer\n",
    "model = RobertaForSequenceClassification.from_pretrained('./saved_model')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./saved_model')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Ensure the model is on the GPU\n",
    "model.to('cuda')\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned tweets dataframe\n",
    "filtered_df = pd.read_csv('cleaned_llm_tweets.csv')\n",
    "filtered_df = filtered_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the token limit\n",
    "TOKEN_LIMIT = 512\n",
    "\n",
    "# Function to preprocess and aggregate tweets with a progress bar\n",
    "def create_aggregates(df, tokenizer, token_limit, num_aggregates=30, eval=False):\n",
    "    aggregates = []\n",
    "    \n",
    "    # Group by MatchID and PeriodID\n",
    "    for (match_id, period_id), group in tqdm(df.groupby(['MatchID', 'PeriodID']), desc=\"Aggregating Tweets\"):\n",
    "        tweets = group['Tweet'].tolist()\n",
    "        if not eval:\n",
    "            event_type = group['EventType'].iloc[0]  # Binary target for the period\n",
    "\n",
    "        # Generate aggregates for this group\n",
    "        for _ in range(num_aggregates):\n",
    "            random.shuffle(tweets)  # Shuffle tweets for randomness\n",
    "            aggregate = \"\"\n",
    "            token_count = 0\n",
    "\n",
    "            for tweet in tweets:\n",
    "                # Tokenize tweet and count tokens\n",
    "                tokenized_tweet = tokenizer.encode(tweet, add_special_tokens=False)\n",
    "                if token_count + len(tokenized_tweet) > token_limit:\n",
    "                    break  # Stop adding tweets if token limit is reached\n",
    "                \n",
    "                # Add the tweet to the aggregate\n",
    "                aggregate += tweet + \" \"\n",
    "                token_count += len(tokenized_tweet)\n",
    "\n",
    "            if not eval:\n",
    "                # Save the aggregate and its label\n",
    "                aggregates.append({\n",
    "                    'text': aggregate.strip(),\n",
    "                    'label': event_type,\n",
    "                    'match_id': match_id,\n",
    "                    'period_id': period_id\n",
    "                })\n",
    "            else:\n",
    "                # Save the aggregate\n",
    "                aggregates.append({\n",
    "                    'text': aggregate.strip(),\n",
    "                    'match_id': match_id,\n",
    "                    'period_id': period_id\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(aggregates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating Tweets: 100%|██████████| 2137/2137 [03:00<00:00, 11.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create aggregates\n",
    "aggregates_df = create_aggregates(filtered_df, tokenizer, TOKEN_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=TOKEN_LIMIT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 64110/64110 [01:29<00:00, 720.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(aggregates_df)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making Predictions: 100%|██████████| 4007/4007 [23:43<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=16)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Making Predictions\"):\n",
    "        inputs = {key: batch[key].to(model.device) for key in ['input_ids', 'attention_mask']}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=-1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "# Add predictions to the aggregates DataFrame\n",
    "aggregates_df['predicted_label'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions over aggregates to CSV\n",
    "aggregates_df.to_csv('llm_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions over aggregates to CSV without text\n",
    "aggregates_df.drop(columns=['text']).to_csv('llm_predictions_no_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 86.96%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate overall accuracy over aggregates\n",
    "accuracy = accuracy_score(aggregates_df['label'], aggregates_df['predicted_label'])\n",
    "print(f'Overall Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating predictions over kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation set\n",
    "import os\n",
    "\n",
    "eval_df = pd.read_csv('cleaned_eval_tweets.csv')\n",
    "eval_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating Tweets: 100%|██████████| 516/516 [00:42<00:00, 12.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate aggregates for evaluation tweets\n",
    "eval_aggregates_df = create_aggregates(eval_df, tokenizer, TOKEN_LIMIT, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15480/15480 [00:21<00:00, 707.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate predictions using model\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(eval_aggregates_df)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making Predictions: 100%|██████████| 484/484 [05:42<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataloader = DataLoader(tokenized_eval_dataset, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Making Predictions\"):\n",
    "        inputs = {key: batch[key].to(model.device) for key in ['input_ids', 'attention_mask']}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=-1).cpu().tolist()\n",
    "        predictions.extend(batch_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the aggregates DataFrame\n",
    "eval_aggregates_df['predicted_label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation predictions to CSV\n",
    "eval_aggregates_df.to_csv('eval_aggregates_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
